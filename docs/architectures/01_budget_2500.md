# Architecture Option A: $2,500 Machine Budget

## Goal

Minimize upfront cost while keeping a usable team SSH node with 10Gb networking and predictable knowledge-base throughput.

## Target Infrastructure Profile

- 1 shared research node
- 10Gb Ethernet
- 12-16 CPU cores
- 64-96 GB RAM
- 2 TB NVMe SSD
- GPU or unified-memory profile sufficient for quantized local models
- SSH access for small team (3-6 users)

This design is OS-agnostic because serving standardizes on `llama-server` (OpenAI-compatible API).

## Logical Architecture

- `gateway` for routing and budget limits
- `llama-server` for local inference
- `redis` for semantic/prefix cache keys and job state
- `qdrant` for retrieval memory by sector
- `kb-writer` service writing canonical records to `SQLite + Parquet`
- `obsidian-projector` job generating optional vault views

## Deployment Pattern

- Single-node compose stack
- One local inference worker pool
- Cloud used for hard tasks and capacity overflow

## Capacity Assumption

- Effective local capacity target: $\mu = 90$ tokens/sec
- Per-source token load: $T_{in}+T_{out}=725{,}000$

Theoretical local-only source throughput:

```math
\text{sources/day} = \frac{\mu \cdot 86400}{725000} \approx 10.73
```

## Cloud Cost Model (Instantiated)

Using the shared model in `00_cloud_cost_model.md`:

- Hard-task cloud fraction: $r_h = 0.20$
- Baseline peak demand: $\lambda_{peak}=145.45$ tokens/sec
- Stress peak demand: $\lambda_{peak}=261.81$ tokens/sec

Overflow fractions:

```math
r_{over,base} = \max\left(0,\frac{145.45-90}{145.45}\right)=0.3812
```

```math
r_{over,stress} = \max\left(0,\frac{261.81-90}{261.81}\right)=0.6562
```

Monthly cloud cost:

```math
C_{month}=S\left(r_hc_h+(1-r_h)r_{over}c_e\right)
```

with $c_e=0.33925$, $c_h=1.69625$.

Baseline (`S=130`):

```math
C_{base}=130\left(0.20\cdot1.69625+0.80\cdot0.3812\cdot0.33925\right)=\$57.55/month
```

Stress (`S=234`):

```math
C_{stress}=234\left(0.20\cdot1.69625+0.80\cdot0.6562\cdot0.33925\right)=\$121.07/month
```

Approximate annual cloud range from this model: `$690 - $1,453`.

## Fit for Project

### Pros

- Lowest hardware cost.
- Fastest to acquire and deploy.
- Works for pilot and steady moderate throughput.

### Constraints

- Highest overflow pressure during peak periods.
- Greater reliance on cloud to maintain throughput.
- Less headroom for larger local models and concurrency.

## Recommended If

Choose this if your priority is immediate delivery with minimal capital expense and you accept higher cloud overflow during busy weeks.
