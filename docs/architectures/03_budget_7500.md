# Architecture Option C: `$7,500` Machine Budget

## Goal

Maximize local capacity and minimize cloud reliance for long-term institutional use across multiple research cycles.

## Target Infrastructure Profile

- 1 primary research node
- 10Gb Ethernet
- 24-32 CPU cores
- 192-256 GB RAM target
- 8 TB NVMe SSD (prefer mirrored layout)
- High-memory GPU or equivalent unified-memory class for larger local model serving
- SSH access for small team (up to ~10 concurrent users with queue controls)

Service layer stays OS-agnostic because inference is exposed through `llama-server`.

## Logical Architecture

- `gateway` with strict budgets and deterministic routing tags
- `llama-server` local inference pool (high-capacity primary + optional secondary worker)
- `redis` for caching and queue metadata
- `qdrant` for sector memory retrieval
- `kb-writer` to canonical `SQLite + Parquet`
- `obsidian-projector` to generate track-aligned vault views
- scheduled backup job for store snapshots

## Deployment Pattern

- Single high-capacity node with low expected overflow
- Cloud used only for selective high-reasoning requests and cross-check workflows

## Memory-Constrained Operating Point

Assumptions for Track A generation profile:

- Installed memory envelope: $M_{avail} = 192 \text{ to } 256$ GB
- Safety factor: $\eta = 0.85$
- Usable memory envelope: $\eta M_{avail} = 163.2 \text{ to } 217.6$ GB
- Planned runtime working set (large quantized model class, higher concurrency): $M_{req} \approx 70 \text{ to } 140$ GB

Throughput decomposition:

- $\mu_{compute} \approx 380$ tokens/sec
- $\mu_{memory} \approx 320$ tokens/sec
- $f_{fit} = 1.00$

```math
\mu_{eff}=f_{fit}\cdot\min(\mu_{compute},\mu_{memory})=1.00\cdot 320=320
```

## Capacity Assumption

- Effective local capacity target: $\mu_{eff} = 320$ tokens/sec
- Per-source token load: $725{,}000$

Theoretical local-only source throughput:

```math
\text{sources/day} = \frac{\mu_{eff} \cdot 86400}{725000} \approx 38.18
```

## Cloud Cost Model (Instantiated)

Using the shared model in `00_cloud_cost_model.md`:

- Hard-task cloud fraction: $r_h = 0.08$
- Baseline peak demand: $44.75$ tokens/sec
- Stress peak demand: $80.56$ tokens/sec

Overflow fractions:

```math
r_{over,base}=0, \quad r_{over,stress}=0
```

Monthly cloud cost:

```math
C_{month}=S\left(r_hc_h+(1-r_h)r_{over}c_e\right)
```

For web-aware enrichment workflows, use the extended cost equation in `00_cloud_cost_model.md`:
$C_{month}^{web}$ with $h_{db}$, $p_{escalate}$, and $c_{fill}$.

with $c_e=0.33925$, $c_h=1.69625$.

Baseline (`S=40`):

```math
C_{base}=40\left(0.08\cdot1.69625\right)=\$5.43/month
```

Stress (`S=72`):

```math
C_{stress}=72\left(0.08\cdot1.69625\right)=\$9.77/month
```

Approximate annual cloud range from this model: `$65 - $117`.

## Test Deployment Snapshot Entries

Use one row per pilot snapshot window (recommended: 24h to 168h windows).

| Snapshot ID | Git Commit | Start (UTC) | End (UTC) | Model Profile | Context Tokens | Concurrency | Sources Completed | Tokens In | Tokens Out | Peak Arrival `lambda_peak_obs` (tok/s) | `mu_compute_obs` (tok/s) | `mu_memory_obs` (tok/s) | `f_fit_obs` | `mu_eff_obs` (tok/s) | `rho_obs` | `r_over_obs` | Peak Memory (GB) | `h_db_obs` | `local_time_budget_sec` | `p_escalate_obs` | `c_fill_obs` (USD/call) | `cloud_fill_calls_obs` | `avg_missing_fields_obs` | `local_cli_calls_obs` | Cloud Spend (USD) | `q_accept_obs` | Notes |
| ----------- | ---------- | ----------- | --------- | ------------- | -------------- | ----------- | ----------------- | --------- | ---------- | -------------------------------------- | ------------------------ | ----------------------- | ----------- | -------------------- | --------- | ------------ | ---------------- | ---------- | ----------------------- | ---------------- | ----------------------- | ---------------------- | ------------------------ | --------------------- | ----------------- | -------------- | ----- |
| snap-001    |            |             |           |               |                |             |                   |           |            |                                        |                          |                         |             |                      |           |              |                  |            |                         |                  |                         |                        |                          |                       |                   |                |       |

Derived fields for this architecture:

```math
\mu_{eff,obs}=f_{fit,obs}\cdot\min(\mu_{compute,obs},\mu_{memory,obs})
```

```math
\rho_{obs}=\frac{\lambda_{peak,obs}}{\mu_{eff,obs}}, \quad r_{over,obs}=\max\left(0,\frac{\lambda_{peak,obs}-\mu_{eff,obs}}{\lambda_{peak,obs}}\right)
```

```math
N_{cloud\_calls,obs}=S_{obs}\left(0.08+0.92\left(r_{over,obs}+(1-r_{over,obs})(1-h_{db,obs})p_{escalate,obs}\right)\right)
```

```math
C_{month,obs}^{web}=S_{obs}\left(0.08\cdot c_h + 0.92\left(r_{over,obs}\cdot c_e + (1-r_{over,obs})(1-h_{db,obs})p_{escalate,obs}c_{fill,obs}\right)\right)
```

Model versus observed quick check:

| Metric                          |  Model | Observed |
| ------------------------------- | -----: | -------: |
| `mu_eff` (tok/s)                |  `320` |          |
| `rho` at baseline               | `0.14` |          |
| Cloud cost/month baseline (USD) | `5.43` |          |

## Fit for Project

### Pros

- Highest local throughput and lowest cloud reliance.
- Better for future institutional expansion and larger local models.
- Lowest risk of queue-driven overflow during deadlines.

### Constraints

- Highest upfront spend.
- More hardware to manage and monitor over time.

## Recommended If

Choose this if the machine is intended as a durable institutional asset across multiple future projects with minimal cloud dependence.
