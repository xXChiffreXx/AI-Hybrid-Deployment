# Architecture Option C: $7,500 Machine Budget

## Goal

Maximize local capacity and minimize cloud reliance for long-term institutional use across multiple research cycles.

## Target Infrastructure Profile

- 1 primary research node
- 10Gb Ethernet
- 24-32 CPU cores
- 192-256 GB RAM target
- 8 TB NVMe SSD (prefer mirrored layout)
- High-memory GPU or equivalent unified-memory class for larger local model serving
- SSH access for small team (up to ~10 concurrent users with queue controls)

Service layer stays OS-agnostic because inference is exposed through `llama-server`.

## Logical Architecture

- `gateway` with strict budgets and deterministic routing tags
- `llama-server` local inference pool (high-capacity primary + optional secondary worker)
- `redis` for caching and queue metadata
- `qdrant` for sector memory retrieval
- `kb-writer` to canonical `SQLite + Parquet`
- `obsidian-projector` to generate track-aligned vault views
- scheduled backup job for store snapshots

## Deployment Pattern

- Single high-capacity node with low expected overflow
- Cloud used only for selective high-reasoning requests and cross-check workflows

## Capacity Assumption

- Effective local capacity target: $\mu = 320$ tokens/sec
- Per-source token load: $725{,}000$

Theoretical local-only source throughput:

$$

\text{sources/day} = \frac{320 \cdot 86400}{725000} \approx 38.18

$$

## Cloud Cost Model (Instantiated)

Using the shared model in `00_cloud_cost_model.md`:

- Hard-task cloud fraction: $r_h = 0.08$
- Baseline peak demand: $145.45$ tokens/sec
- Stress peak demand: $261.81$ tokens/sec

Overflow fractions:

$$

r_{over,base}=0, \quad r_{over,stress}=0

$$

Monthly cloud cost:

$$

C_{month}=S\left(r_hc_h+(1-r_h)r_{over}c_e\right)

$$

with $c_e=0.33925$, $c_h=1.69625$.

Baseline (`S=130`):

$$

C_{base}=130\left(0.08\cdot1.69625\right)=\$17.64/month

$$

Stress (`S=234`):

$$

C_{stress}=234\left(0.08\cdot1.69625\right)=\$31.75/month

$$

Approximate annual cloud range from this model: `$212 - $381`.

## Fit for Project

### Pros

- Highest local throughput and lowest cloud reliance.
- Better for future institutional expansion and larger local models.
- Lowest risk of queue-driven overflow during deadlines.

### Constraints

- Highest upfront spend.
- More hardware to manage and monitor over time.

## Recommended If

Choose this if the machine is intended as a durable institutional asset across multiple future projects with minimal cloud dependence.
